# ğŸ–¼ï¸ Image Caption Generator with Attention Mechanism

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A deep learning project that automatically generates natural language descriptions for images using an encoder-decoder architecture with Bahdanau attention mechanism.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Acknowledgments](#acknowledgments)

## âœ¨ Features

- **Attention Mechanism**: Bahdanau (additive) attention for focusing on relevant image regions
- **Pre-trained Encoder**: ResNet-50 (ImageNet) for robust feature extraction
- **Spatial Features**: 7Ã—7 grid (49 regions) for fine-grained attention
- **Interactive GUI**: Gradio-based web interface for easy caption generation
- **Comprehensive Evaluation**: BLEU-1/2/3/4 metrics with visual reports
- **Training Visualization**: Real-time loss curves and performance tracking

## ğŸ—ï¸ Architecture

### Encoder
- **Model**: ResNet-50 (pretrained on ImageNet)
- **Output**: 49 spatial regions (7Ã—7 grid)
- **Feature Dimension**: 2048 per region

### Attention Mechanism
- **Type**: Bahdanau (Additive) Attention
- **Attention Dim**: 512
- **Purpose**: Dynamic focus on relevant image regions per word

### Decoder
- **Model**: LSTM with attention
- **Embedding Dim**: 512
- **Hidden Dim**: 512
- **Vocabulary Size**: 2,590 words
- **Parameters**: 13.4M

## ğŸ“Š Results

### Performance on Flickr8k Test Set (1,000 unseen images)

| Metric | Score | Industry Baseline |
|--------|-------|-------------------|
| BLEU-1 | 0.647 | 0.50-0.60 |
| BLEU-2 | 0.443 | 0.30-0.40 |
| BLEU-3 | 0.306 | 0.18-0.25 |
| BLEU-4 | 0.208 | 0.10-0.15 |

**Our model outperforms typical baselines on all metrics!**

### Training Performance
- Training Loss: 2.22 (final)
- Validation Loss: 3.04 (best)
- Training Time: ~25 minutes (15 epochs on RTX 3060)
- GPU Utilization: 80-95%

## ğŸš€ Installation

### Prerequisites
- Python 3.12+
- CUDA-capable GPU (recommended)
- 8GB+ RAM
- 10GB+ disk space

### Setup Steps

1. Clone the repository
git clone https://github.com/Triplejw/image-caption-generator.git
cd image-caption-generator

3. Create virtual environment
python -m venv venv
source venv/bin/activate

4. Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt

5. Download Flickr8k dataset
mkdir -p ~/.kaggle
chmod 600 ~/.kaggle/kaggle.json
kaggle datasets download -d adityajn105/flickr8k
unzip flickr8k.zip -d data

## ğŸ’» Usage

### Training from Scratch

Extract features, create splits, then train:
python extract_features.py
python create_splits.py
python train_v2_attention.py

### Generating Captions

Command line:
python generate_caption_v2.py path/to/image.jpg
python generate_caption_v2.py path/to/image.jpg --beam

Web GUI:
python app_gui.py

### Evaluation

python evaluate_model.py
python test_evaluation_detailed.py
python create_visual_report.py

## ğŸ“ Project Structure

image-caption-generator/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ Images/
â”‚ â”œâ”€â”€ captions.txt
â”‚ â”œâ”€â”€ features/
â”‚ â””â”€â”€ splits/
â”œâ”€â”€ models_v2/
â”‚ â”œâ”€â”€ best_model.pth
â”‚ â””â”€â”€ vocabulary.pkl
â”œâ”€â”€ train_v2_attention.py
â”œâ”€â”€ extract_features.py
â”œâ”€â”€ create_splits.py
â”œâ”€â”€ generate_caption_v2.py
â”œâ”€â”€ evaluate_model.py
â”œâ”€â”€ app_gui.py
â””â”€â”€ README.md

## ğŸ“ Requirements

torch>=2.5.0
torchvision>=0.20.0
gradio>=4.0.0
pandas>=2.0.0
numpy>=1.24.0
nltk>=3.8.0
pillow>=10.0.0
matplotlib>=3.7.0
tqdm>=4.65.0
scikit-learn>=1.3.0
kaggle>=1.5.16

## ğŸ› ï¸ Technical Stack

- Deep Learning: PyTorch 2.5
- Computer Vision: ResNet-50
- NLP: NLTK, BLEU metrics
- GUI: Gradio
- Hardware: NVIDIA RTX 3060

## ğŸ¤ Acknowledgments

- Dataset: Flickr8k from Kaggle
- Architecture: "Show, Attend and Tell" (Xu et al., 2015)
- Pre-trained Model: ResNet-50

## ğŸ“„ License

MIT License

## ğŸ‘¤ Author

**Joshua JJ Wonder**
- GitHub: @Triplejw
- Email: wonderjj2017@gmail.com

---

Built with â¤ï¸ using PyTorch and Attention

